# Week 4. Clustering and classification

```{r}
date()
```

```{r, message=FALSE}
# Packages
library(MASS); library(corrplot); library(ggplot2); library(GGally); library(tidyr)
```

### 2. Load data and explore

```{r}
# Load data

data("Boston")
```

This weeks exercise uses the Boston dataset from the MASS package. The dataset contains information related to values of housing in different suburbs of Boston. It includes variables such as per capita crime rate (the main focus here in the exercise), average number of rooms per dwelling and distances to employment centres, for example. More info available at: [https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html).

```{r}
# Explore dataset
str(Boston)
```

The data has 506 observations and 14 variables. Two of the variables are integer - chas: dummy variable describing if tract bounds river or not, and rad: index of accessibility to radial highways. Others variables are numerical.

```{r}
# Summaries of the variables
summary(Boston)
```

### 3. Graphical overview

```{r, out.width = '130%'}
# Plot matrix

library(GGally)
ggpairs(data = Boston,
        upper = list(continuous = wrap("cor", size=1.8)),
        lower = list(continuous = wrap("points", alpha = 0.3, size=0.3)), progress = F)
```

```{r}
cor_plot <- cor(Boston) %>% 
  round(digits = 2) %>% 
  corrplot(method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```

The plot matrix and correlations plot show that crime rate is positively most strongly correlated with index of accessibility to radial highways (rad) and full-value property-tax rate (tax). There is also a strong positive correlation between these two variables (rad and tax). Distributions of the variables mainly do not follow normal distributions, some are skewed, some have multiple peaks, for example.

### 4. Standardization and train and test datasets

```{r}
# Standardize

boston_scaled <- as.data.frame(scale(Boston))
summary(boston_scaled) #summaries of the scaled variables
```

After standardization all variables are centered to have a mean value of 0. 

```{r}
# Crime rate into categorical variable

bins <- quantile(boston_scaled$crim) #quantiles of crim, to be used as break points
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high")) #categorical 'crime' variable
table(crime)
boston_scaled <- dplyr::select(boston_scaled, -crim) #drop old crime rate variable
boston_scaled <- data.frame(boston_scaled, crime) #add new crime rate variable
```

```{r}
# Divide dataset to train and test sets

n <- nrow(boston_scaled) #number of rows in scaled Boston dataset 
ind <- sample(n,  size = n * 0.8) #choose randomly 80% of the 506 rows
train <- boston_scaled[ind,] #create train dataset
test <- boston_scaled[-ind,] #create test dataset
```

### 5. Linear discriminant analysis (LDA) and (bi)plot

LDA is fit using the categorical crime rate as the target variable and all the other variables as predictor variables.

```{r}
# Fit a linear discriminant analysis

lda.fit <- lda(crime ~ ., data = train)
lda.fit
```

LDA is a technique used to reduce the number of dimensions in a dataset. In practice, it looks for linear combinations of original variables. Here, three dimensions LD1, LD2 and LD3 are found.

```{r}
# LDA (bi)plot

lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

classes <- as.numeric(train$crime) #crime classes as numeric

plot(lda.fit, dimen = 2, col = classes, pch = classes) #plot
lda.arrows(lda.fit, myscale = 2)
```

### 6. Prediction

```{r}
correct_classes <- test$crime #save crime classes from test dataset
test <- dplyr::select(test, -crime) #remove  crime variable from test dataset

lda.pred <- predict(lda.fit, newdata = test) #predict crime rate with test data

table(correct = correct_classes, predicted = lda.pred$class) #tabulate results
prop.table(table(correct = correct_classes, predicted = lda.pred$class), 1)
```

LDA is used to predict the class of the given observations. The model predicted around 70% right for the low-crime category, a bit lower in the med_low and med_high and a lot better in the high crime areas.

### 7. K-means clustering

```{r}
# Data
data("Boston") #reload data
Boston <- as.data.frame(scale(Boston)) #standardize the dataset
```

```{r}
# Distances between the observations

dist_eu <- dist(Boston) #euclidean distance matrix
summary(dist_eu)

dist_man <- dist(Boston, method = "manhattan") #manhattan distance matrix
summary(dist_man)
```

The median euclidean distance is 4.8.

```{r}
# K-means clustering
set.seed(13)
km <- kmeans(Boston, centers = 3)
pairs(Boston[1:6], col = km$cluster) #plot Boston dataset with clusters identified with colors
```

```{r}
# Optimal number of clusters - total within cluster sum of squares (WCSS)
set.seed(123)
k_max <- 10 #max number of clusters
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss}) #calculate total within sum of squares by number of clusters
qplot(x = 1:k_max, y = twcss, geom = 'line') #plot number of clusters and the total WCSS
```

The optimal number of clusters can be assessed comparing the total within sum of squares for different numbers of clusters. The plot shows the deepest drop in total within sum of squares when passing from 1 to 2.

```{r, warning=F, out.width = '130%'}
# K-means clustering - second attempt with two clusters
set.seed(13)
km <- kmeans(Boston, centers = 2)
pairs(Boston, col = km$cluster) #plot Boston dataset with clusters identified with colors

#km$cluster <- as.factor(km$cluster)

ggpairs(data = Boston,
        mapping = aes(color = as.factor(km$cluster), alpha = 0.5),
        upper = list(continuous = wrap("cor", size=1.8)),
        lower = list(continuous = wrap("points", alpha = 0.3, size=0.3)), progress = F)
```

